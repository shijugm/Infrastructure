Reference : https://towardsdatascience.com/a-journey-into-big-data-with-apache-spark-part-1-5dfcc2bccdd2

#######################################################################################
# this is the command to build the image once the image is built this is not needed
#######################################################################################
docker build -t shiju/spark:latest .

#######################################################################################
# Run the command 
#######################################################################################

# create a network for master and slave
#########
docker network create spark_network


# Start the spark master
########
docker run --rm -it --name spark-master --hostname spark-master -p 7077:7077 -p 8080:8080  --network spark_network  shiju/spark:latest /bin/sh

# Run this in the master
###
/spark/bin/spark-class org.apache.spark.deploy.master.Master --ip `hostname` --port 7077 --webui-port 8080


# Start the spark slave
########

docker run --rm -it --name spark-worker --hostname spark-worker --network spark_network shiju/spark:latest /bin/sh

# Run this on the slave to register
###
/spark/bin/spark-class org.apache.spark.deploy.worker.Worker --webui-port 8080 spark://spark-master:7077


# Refer the docker compose to finish all this in a script


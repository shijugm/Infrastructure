Reference : https://towardsdatascience.com/a-journey-into-big-data-with-apache-spark-part-1-5dfcc2bccdd2

#######################################################################################
# this is the command to build the image once the image is built this is not needed
#######################################################################################


# need to check why the '.' is needed in the end of the command.

sudo docker build -f $DOCKER_IMAGE_PATH/spark/Dockerfile -t shiju/spark:latest .

#######################################################################################
# Run the command 
#######################################################################################

# create a network for master and slave
#########
sudo docker network create spark_network


# Start the spark master
########
sudo docker run --rm -it --name spark-master --hostname spark-master -p 7077:7077 -p 8081:8081  --network spark_network  shiju/spark:latest /bin/bash

# Run this in the master
###
/spark/bin/spark-class org.apache.spark.deploy.master.Master --ip `hostname` --port 7077 --webui-port 8081


# Start the spark slave
########

sudo docker run --rm -it --name spark-worker --hostname spark-worker --network spark_network shiju/spark:latest /bin/bash

# Run this on the slave to register
###
/spark/bin/spark-class org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077

#REPEAT above slave commands for more slave nodes
# change the name each time


# create a node to submit the job. This container is not registered as a worker node
sudo docker run --rm -it --network spark_network --name spark-submit-driver -v  /home/shiju/workarea/git_repositories/spark_python:/src -v  /home/shiju/workarea/git_repositories/datasets:/data shiju/spark:latest  /bin/bash


# spark UI : localhost:8081/


# Refer the docker compose to finish all this in a script

